"""
Ø§Ø®ØªØ¨Ø§Ø± Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ - Semantic Search Performance Testing
ÙŠÙ‚ÙŠØ³: Precision, Recall, F1-Score, MRR
"""

import json
import time
import numpy as np
from typing import List, Dict, Tuple
import requests
from pathlib import Path
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±
API_BASE_URL = "http://localhost:8000"
RESULTS_DIR = Path("test_results")
RESULTS_DIR.mkdir(exist_ok=True)


class SearchPerformanceTester:
    """Ø§Ø®ØªØ¨Ø§Ø± Ø´Ø§Ù…Ù„ Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ"""

    def __init__(self, api_url: str = API_BASE_URL):
        self.api_url = api_url
        self.test_queries = []
        self.results = {
            'precision_at_k': {},
            'recall_at_k': {},
            'f1_at_k': {},
            'mrr': 0.0,
            'response_times': [],
            'detailed_results': []
        }

    def load_test_queries(self, queries_file: str = None):
        """
        ØªØ­Ù…ÙŠÙ„ Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± Ù…Ø¹ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø§Øª Ø§Ù„ØµØ­ÙŠØ­Ø©

        ØµÙŠØºØ© Ø§Ù„Ù…Ù„Ù JSON:
        [
            {
                "query": "Ù…Ø§ Ù‡ÙŠ Ø£Ø­Ø¯Ø« Ø§Ù„ØªØ·ÙˆØ±Ø§Øª ÙÙŠ Ø§Ù„Ø·Ø§Ù‚Ø© Ø§Ù„Ù…ØªØ¬Ø¯Ø¯Ø©ØŸ",
                "language": "arabic",
                "relevant_docs": ["doc1.pdf", "doc5.pdf"],
                "top_relevant": "doc1.pdf"
            }
        ]
        """
        if queries_file and Path(queries_file).exists():
            with open(queries_file, 'r', encoding='utf-8') as f:
                self.test_queries = json.load(f)
        else:
            # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø§Ø®ØªØ¨Ø§Ø± Ù†Ù…ÙˆØ°Ø¬ÙŠØ©
            self.test_queries = self._generate_sample_queries()

        logger.info(f"ØªÙ… ØªØ­Ù…ÙŠÙ„ {len(self.test_queries)} Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø§Ø®ØªØ¨Ø§Ø±")
        return self.test_queries

    def _generate_sample_queries(self) -> List[Dict]:
        """Ø¥Ù†Ø´Ø§Ø¡ Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø§Ø®ØªØ¨Ø§Ø± Ù†Ù…ÙˆØ°Ø¬ÙŠØ©"""
        return [
            {
                "query": "renewable energy developments",
                "language": "english",
                "relevant_docs": [],  # Ø³ÙŠØªÙ… Ù…Ù„Ø¤Ù‡Ø§ ÙŠØ¯ÙˆÙŠÙ‹Ø§
                "top_relevant": None
            },
            {
                "query": "Ø§Ù„ØªØ·ÙˆØ±Ø§Øª Ø§Ù„Ø§Ù‚ØªØµØ§Ø¯ÙŠØ© ÙÙŠ Ø§Ù„Ù…Ù†Ø·Ù‚Ø©",
                "language": "arabic",
                "relevant_docs": [],
                "top_relevant": None
            },
            # ÙŠÙ…ÙƒÙ† Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù…Ø²ÙŠØ¯...
        ]

    def search(self, query: str, top_k: int = 5, language: str = None) -> Tuple[List[Dict], float]:
        """
        Ø¥Ø¬Ø±Ø§Ø¡ Ø¨Ø­Ø« Ø¹Ø¨Ø± API

        Returns:
            (results, response_time)
        """
        start_time = time.time()

        try:
            response = requests.post(
                f"{self.api_url}/api/search",
                json={
                    "query": query,
                    "search_mode": "unified",
                    "top_k": top_k,
                    "language": language
                },
                timeout=30
            )

            response_time = time.time() - start_time

            if response.status_code == 200:
                data = response.json()
                return data.get('results', []), response_time
            else:
                logger.error(f"ÙØ´Ù„ Ø§Ù„Ø¨Ø­Ø«: {response.status_code}")
                return [], response_time

        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø¨Ø­Ø«: {e}")
            response_time = time.time() - start_time
            return [], response_time

    def calculate_precision_at_k(self, retrieved: List[str], relevant: List[str], k: int) -> float:
        """
        Ø­Ø³Ø§Ø¨ Precision@k

        Precision@k = |Relevant âˆ© Retrieved@k| / k
        """
        if not retrieved or k == 0:
            return 0.0

        retrieved_at_k = set(retrieved[:k])
        relevant_set = set(relevant)

        intersection = retrieved_at_k & relevant_set

        return len(intersection) / k

    def calculate_recall_at_k(self, retrieved: List[str], relevant: List[str], k: int) -> float:
        """
        Ø­Ø³Ø§Ø¨ Recall@k

        Recall@k = |Relevant âˆ© Retrieved@k| / |Relevant|
        """
        if not relevant:
            return 0.0

        retrieved_at_k = set(retrieved[:k])
        relevant_set = set(relevant)

        intersection = retrieved_at_k & relevant_set

        return len(intersection) / len(relevant_set)

    def calculate_f1_at_k(self, precision: float, recall: float) -> float:
        """
        Ø­Ø³Ø§Ø¨ F1-Score@k

        F1@k = 2 Ã— (Precision@k Ã— Recall@k) / (Precision@k + Recall@k)
        """
        if precision + recall == 0:
            return 0.0

        return 2 * (precision * recall) / (precision + recall)

    def calculate_mrr(self, retrieved: List[str], relevant: List[str]) -> float:
        """
        Ø­Ø³Ø§Ø¨ Mean Reciprocal Rank

        MRR = 1 / rank_of_first_relevant
        """
        for i, doc in enumerate(retrieved, 1):
            if doc in relevant:
                return 1.0 / i

        return 0.0

    def run_tests(self, k_values: List[int] = [1, 3, 5, 10]):
        """
        ØªØ´ØºÙŠÙ„ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª
        """
        logger.info("="*80)
        logger.info("Ø¨Ø¯Ø¡ Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ")
        logger.info("="*80)

        # ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        for k in k_values:
            self.results['precision_at_k'][k] = []
            self.results['recall_at_k'][k] = []
            self.results['f1_at_k'][k] = []

        mrr_scores = []

        # ØªØ´ØºÙŠÙ„ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ù„ÙƒÙ„ Ø§Ø³ØªØ¹Ù„Ø§Ù…
        for idx, test_case in enumerate(self.test_queries, 1):
            query = test_case['query']
            relevant_docs = test_case.get('relevant_docs', [])
            language = test_case.get('language')

            logger.info(f"\n[{idx}/{len(self.test_queries)}] Ø§Ø®ØªØ¨Ø§Ø±: {query[:50]}...")

            # Ø¥Ø¬Ø±Ø§Ø¡ Ø§Ù„Ø¨Ø­Ø«
            max_k = max(k_values)
            results, response_time = self.search(query, top_k=max_k, language=language)

            # Ø­ÙØ¸ Ø²Ù…Ù† Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø©
            self.results['response_times'].append(response_time)

            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹Ø©
            retrieved_docs = [r['source_file'] for r in results]

            # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³ Ù„ÙƒÙ„ k
            query_metrics = {
                'query': query,
                'language': language,
                'relevant_count': len(relevant_docs),
                'response_time': response_time,
                'metrics': {}
            }

            for k in k_values:
                precision = self.calculate_precision_at_k(retrieved_docs, relevant_docs, k)
                recall = self.calculate_recall_at_k(retrieved_docs, relevant_docs, k)
                f1 = self.calculate_f1_at_k(precision, recall)

                self.results['precision_at_k'][k].append(precision)
                self.results['recall_at_k'][k].append(recall)
                self.results['f1_at_k'][k].append(f1)

                query_metrics['metrics'][f'k={k}'] = {
                    'precision': precision,
                    'recall': recall,
                    'f1': f1
                }

                logger.info(f"  k={k}: P={precision:.3f}, R={recall:.3f}, F1={f1:.3f}")

            # Ø­Ø³Ø§Ø¨ MRR
            mrr = self.calculate_mrr(retrieved_docs, relevant_docs)
            mrr_scores.append(mrr)
            query_metrics['mrr'] = mrr

            self.results['detailed_results'].append(query_metrics)

        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…ØªÙˆØ³Ø·Ø§Øª
        self.results['avg_precision_at_k'] = {
            k: np.mean(scores) for k, scores in self.results['precision_at_k'].items()
        }
        self.results['avg_recall_at_k'] = {
            k: np.mean(scores) for k, scores in self.results['recall_at_k'].items()
        }
        self.results['avg_f1_at_k'] = {
            k: np.mean(scores) for k, scores in self.results['f1_at_k'].items()
        }
        self.results['mrr'] = np.mean(mrr_scores) if mrr_scores else 0.0

        self.results['avg_response_time'] = np.mean(self.results['response_times'])
        self.results['std_response_time'] = np.std(self.results['response_times'])

        logger.info("\n" + "="*80)
        logger.info("Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©")
        logger.info("="*80)
        self.print_summary()

        return self.results

    def print_summary(self):
        """Ø·Ø¨Ø§Ø¹Ø© Ù…Ù„Ø®Øµ Ø§Ù„Ù†ØªØ§Ø¦Ø¬"""
        print("\nğŸ“Š Ù…Ù„Ø®Øµ Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ\n")

        print("Ø¬Ø¯ÙˆÙ„: Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø¹Ù†Ø¯ Ù‚ÙŠÙ… k Ù…Ø®ØªÙ„ÙØ©")
        print("-" * 70)
        print(f"{'Ø§Ù„Ù…Ù‚ÙŠØ§Ø³':<20} {'k=1':<12} {'k=3':<12} {'k=5':<12} {'k=10':<12}")
        print("-" * 70)

        # Precision
        precision_row = "Precision"
        for k in [1, 3, 5, 10]:
            if k in self.results['avg_precision_at_k']:
                precision_row += f"{self.results['avg_precision_at_k'][k]:>12.3f}"
        print(precision_row)

        # Recall
        recall_row = "Recall"
        for k in [1, 3, 5, 10]:
            if k in self.results['avg_recall_at_k']:
                recall_row += f"{self.results['avg_recall_at_k'][k]:>12.3f}"
        print(recall_row)

        # F1-Score
        f1_row = "F1-Score"
        for k in [1, 3, 5, 10]:
            if k in self.results['avg_f1_at_k']:
                f1_row += f"{self.results['avg_f1_at_k'][k]:>12.3f}"
        print(f1_row)

        print("-" * 70)
        print(f"\nMRR (Mean Reciprocal Rank): {self.results['mrr']:.3f}")
        print(f"\nÙ…ØªÙˆØ³Ø· Ø²Ù…Ù† Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø©: {self.results['avg_response_time']:.3f} Ø«Ø§Ù†ÙŠØ©")
        print(f"Ø§Ù„Ø§Ù†Ø­Ø±Ø§Ù Ø§Ù„Ù…Ø¹ÙŠØ§Ø±ÙŠ: {self.results['std_response_time']:.3f} Ø«Ø§Ù†ÙŠØ©")
        print(f"Ø¹Ø¯Ø¯ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø§Ù„Ù…Ø®ØªØ¨Ø±Ø©: {len(self.test_queries)}")

    def save_results(self, filename: str = "search_performance_results.json"):
        """Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø¥Ù„Ù‰ Ù…Ù„Ù JSON"""
        output_path = RESULTS_DIR / filename

        # ØªØ­ÙˆÙŠÙ„ numpy types Ø¥Ù„Ù‰ Python types
        results_to_save = {
            'avg_precision_at_k': {k: float(v) for k, v in self.results['avg_precision_at_k'].items()},
            'avg_recall_at_k': {k: float(v) for k, v in self.results['avg_recall_at_k'].items()},
            'avg_f1_at_k': {k: float(v) for k, v in self.results['avg_f1_at_k'].items()},
            'mrr': float(self.results['mrr']),
            'avg_response_time': float(self.results['avg_response_time']),
            'std_response_time': float(self.results['std_response_time']),
            'num_queries': len(self.test_queries),
            'detailed_results': self.results['detailed_results']
        }

        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(results_to_save, f, ensure_ascii=False, indent=2)

        logger.info(f"\nâœ… ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙÙŠ: {output_path}")
        return output_path

    def generate_latex_table(self, filename: str = "search_results_table.tex"):
        """ØªÙˆÙ„ÙŠØ¯ Ø¬Ø¯ÙˆÙ„ LaTeX Ù„Ù„Ø¨Ø­Ø« Ø§Ù„Ø¹Ù„Ù…ÙŠ"""
        output_path = RESULTS_DIR / filename

        latex_content = r"""\begin{table}[h]
\centering
\caption{Ù…Ù‚Ø§ÙŠÙŠØ³ Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ}
\label{tab:search_performance}
\begin{tabular}{|l|c|c|c|c|}
\hline
\textbf{Ø§Ù„Ù…Ù‚ÙŠØ§Ø³} & \textbf{k=1} & \textbf{k=3} & \textbf{k=5} & \textbf{k=10} \\
\hline
"""

        # Precision row
        latex_content += "Precision & "
        latex_content += " & ".join([
            f"{self.results['avg_precision_at_k'].get(k, 0):.2f}"
            for k in [1, 3, 5, 10]
        ])
        latex_content += " \\\\\n"

        # Recall row
        latex_content += "Recall & "
        latex_content += " & ".join([
            f"{self.results['avg_recall_at_k'].get(k, 0):.2f}"
            for k in [1, 3, 5, 10]
        ])
        latex_content += " \\\\\n"

        # F1-Score row
        latex_content += "F1-Score & "
        latex_content += " & ".join([
            f"{self.results['avg_f1_at_k'].get(k, 0):.2f}"
            for k in [1, 3, 5, 10]
        ])
        latex_content += " \\\\\n"

        latex_content += r"""\hline
\end{tabular}
\end{table}

\noindent
MRR (Mean Reciprocal Rank): """ + f"{self.results['mrr']:.2f}"

        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(latex_content)

        logger.info(f"âœ… ØªÙ… Ø­ÙØ¸ Ø¬Ø¯ÙˆÙ„ LaTeX ÙÙŠ: {output_path}")
        return output_path


def main():
    """Ø§Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ"""
    print("\n" + "="*80)
    print("ğŸ” Ø¨Ø±Ù†Ø§Ù…Ø¬ Ø§Ø®ØªØ¨Ø§Ø± Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ")
    print("="*80 + "\n")

    # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø«ÙŠÙ„ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±
    tester = SearchPerformanceTester()

    # ØªØ­Ù…ÙŠÙ„ Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±
    # ÙŠÙ…ÙƒÙ† ØªÙ…Ø±ÙŠØ± Ù…Ù„Ù JSON Ø¨Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª
    tester.load_test_queries("test_data/search_queries.json")

    # ØªØ´ØºÙŠÙ„ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª
    results = tester.run_tests(k_values=[1, 3, 5, 10])

    # Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
    tester.save_results()
    tester.generate_latex_table()

    print("\nâœ… Ø§ÙƒØªÙ…Ù„Øª Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ø¨Ù†Ø¬Ø§Ø­!")


if __name__ == "__main__":
    main()
