"""
ØªÙˆÙ„ÙŠØ¯ ØªÙ‚Ø§Ø±ÙŠØ± Ø¥Ø­ØµØ§Ø¦ÙŠØ© Ø´Ø§Ù…Ù„Ø© - Comprehensive Statistical Report Generator
ÙŠÙˆÙ„Ø¯ ØªÙ‚Ø§Ø±ÙŠØ± Ø¨ØµÙŠØº: JSON, Markdown, LaTeX, HTML
"""

import json
import numpy as np
from pathlib import Path
from datetime import datetime
from typing import Dict, List
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

RESULTS_DIR = Path("test_results")
REPORTS_DIR = Path("reports")
REPORTS_DIR.mkdir(exist_ok=True)


class ReportGenerator:
    """Ù…ÙˆÙ„Ø¯ ØªÙ‚Ø§Ø±ÙŠØ± Ø¥Ø­ØµØ§Ø¦ÙŠØ© Ø´Ø§Ù…Ù„Ø© Ù„Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª"""

    def __init__(self):
        self.data = {
            'search_performance': {},
            'video_analysis': {},
            'writer_extraction': {},
            'performance_metrics': {}
        }
        self.report_date = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    def load_all_results(self):
        """ØªØ­Ù…ÙŠÙ„ Ø¬Ù…ÙŠØ¹ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª"""
        logger.info("ØªØ­Ù…ÙŠÙ„ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª...")

        # 1. Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ
        search_file = RESULTS_DIR / "search_performance_results.json"
        if search_file.exists():
            with open(search_file, 'r', encoding='utf-8') as f:
                self.data['search_performance'] = json.load(f)
            logger.info("âœ“ ØªÙ… ØªØ­Ù…ÙŠÙ„ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ")

        # 2. Ù†ØªØ§Ø¦Ø¬ ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ
        video_file = RESULTS_DIR / "video_analysis_results.json"
        if video_file.exists():
            with open(video_file, 'r', encoding='utf-8') as f:
                self.data['video_analysis'] = json.load(f)
            logger.info("âœ“ ØªÙ… ØªØ­Ù…ÙŠÙ„ Ù†ØªØ§Ø¦Ø¬ ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ")

        # 3. Ù†ØªØ§Ø¦Ø¬ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒØªÙ‘Ø§Ø¨
        writer_file = RESULTS_DIR / "writer_extraction_results.json"
        if writer_file.exists():
            with open(writer_file, 'r', encoding='utf-8') as f:
                self.data['writer_extraction'] = json.load(f)
            logger.info("âœ“ ØªÙ… ØªØ­Ù…ÙŠÙ„ Ù†ØªØ§Ø¦Ø¬ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒØªÙ‘Ø§Ø¨")

        # 4. Ù†ØªØ§Ø¦Ø¬ Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ø£Ø¯Ø§Ø¡
        perf_file = RESULTS_DIR / "performance_metrics.json"
        if perf_file.exists():
            with open(perf_file, 'r', encoding='utf-8') as f:
                self.data['performance_metrics'] = json.load(f)
            logger.info("âœ“ ØªÙ… ØªØ­Ù…ÙŠÙ„ Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ø£Ø¯Ø§Ø¡")

    def generate_markdown_report(self) -> Path:
        """ØªÙˆÙ„ÙŠØ¯ ØªÙ‚Ø±ÙŠØ± Markdown ÙƒØ§Ù…Ù„"""
        logger.info("ØªÙˆÙ„ÙŠØ¯ ØªÙ‚Ø±ÙŠØ± Markdown...")

        output_file = REPORTS_DIR / f"full_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"

        md_content = f"""# ØªÙ‚Ø±ÙŠØ± Ø´Ø§Ù…Ù„ Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ù†Ø¸Ø§Ù… RAG API Ù…Ø¹ ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ

**ØªØ§Ø±ÙŠØ® Ø§Ù„ØªÙ‚Ø±ÙŠØ±:** {self.report_date}

---

## 1. Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ (Semantic Search Performance)

"""

        # Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ
        if self.data['search_performance']:
            sp = self.data['search_performance']

            md_content += """### 1.1 Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ø¯Ù‚Ø© (Accuracy Metrics)

| Ø§Ù„Ù…Ù‚ÙŠØ§Ø³ | k=1 | k=3 | k=5 | k=10 |
|---------|-----|-----|-----|------|
"""
            # Precision
            md_content += "| Precision |"
            for k in [1, 3, 5, 10]:
                val = sp['avg_precision_at_k'].get(str(k), 0)
                md_content += f" {val:.3f} |"
            md_content += "\n"

            # Recall
            md_content += "| Recall |"
            for k in [1, 3, 5, 10]:
                val = sp['avg_recall_at_k'].get(str(k), 0)
                md_content += f" {val:.3f} |"
            md_content += "\n"

            # F1-Score
            md_content += "| F1-Score |"
            for k in [1, 3, 5, 10]:
                val = sp['avg_f1_at_k'].get(str(k), 0)
                md_content += f" {val:.3f} |"
            md_content += "\n"

            md_content += f"""
### 1.2 Ù…Ù‚Ø§ÙŠÙŠØ³ Ø£Ø®Ø±Ù‰

- **MRR (Mean Reciprocal Rank):** {sp.get('mrr', 0):.3f}
- **Ù…ØªÙˆØ³Ø· Ø²Ù…Ù† Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø©:** {sp.get('avg_response_time', 0):.3f} Ø«Ø§Ù†ÙŠØ©
- **Ø§Ù„Ø§Ù†Ø­Ø±Ø§Ù Ø§Ù„Ù…Ø¹ÙŠØ§Ø±ÙŠ:** {sp.get('std_response_time', 0):.3f} Ø«Ø§Ù†ÙŠØ©
- **Ø¹Ø¯Ø¯ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø§Ù„Ù…Ø®ØªØ¨Ø±Ø©:** {sp.get('num_queries', 0)}

---

"""

        # Ù†ØªØ§Ø¦Ø¬ ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ
        md_content += """## 2. Ù†ØªØ§Ø¦Ø¬ ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ (Video Analysis Performance)

"""

        if self.data['video_analysis']:
            va = self.data['video_analysis']

            md_content += f"""### 2.1 Ø¯Ù‚Ø© Ø§Ù„Ù†Ø³Ø® Ø§Ù„ØµÙˆØªÙŠ

| Ø§Ù„Ù…Ù‚ÙŠØ§Ø³ | Ø§Ù„Ù‚ÙŠÙ…Ø© |
|---------|--------|
| WER (Word Error Rate) | {va.get('avg_wer', 0)*100:.2f}% |
| CER (Character Error Rate) | {va.get('avg_cer', 0)*100:.2f}% |

### 2.2 Ø¬ÙˆØ¯Ø© Ø§Ù„ØªÙ„Ø®ÙŠØµ (ROUGE Scores)

| Ø§Ù„Ù…Ù‚ÙŠØ§Ø³ | Ø§Ù„Ø¯Ø±Ø¬Ø© |
|---------|--------|
| ROUGE-1 | {va.get('avg_rouge1', 0):.3f} |
| ROUGE-2 | {va.get('avg_rouge2', 0):.3f} |
| ROUGE-L | {va.get('avg_rougeL', 0):.3f} |

- **Ø¹Ø¯Ø¯ Ø§Ù„ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª Ø§Ù„Ù…Ø®ØªØ¨Ø±Ø©:** {va.get('num_videos', 0)}

---

"""

        # Ù†ØªØ§Ø¦Ø¬ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒØªÙ‘Ø§Ø¨
        md_content += """## 3. Ù†ØªØ§Ø¦Ø¬ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒØªÙ‘Ø§Ø¨ (Writer Extraction Performance)

"""

        if self.data['writer_extraction']:
            we = self.data['writer_extraction']

            md_content += f"""### 3.1 Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬

| Ø§Ù„Ù…Ù‚ÙŠØ§Ø³ | Ø§Ù„Ù‚ÙŠÙ…Ø© |
|---------|--------|
| Precision | {we.get('avg_precision', 0)*100:.2f}% |
| Recall | {we.get('avg_recall', 0)*100:.2f}% |
| F1-Score | {we.get('avg_f1', 0)*100:.2f}% |

### 3.2 Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø© Ø§Ù„Ø¶Ø¨Ø§Ø¨ÙŠØ© (Fuzzy Matching)

"""

            if we.get('fuzzy_matching_results'):
                md_content += "| Ø§Ù„Ø¹ØªØ¨Ø© | Ø§Ù„Ø¯Ù‚Ø© |\n|--------|-------|\n"
                for result in we['fuzzy_matching_results']:
                    md_content += f"| {result['threshold']:.2f} | {result['accuracy']*100:.2f}% |\n"

            md_content += f"\n- **Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ù…Ø®ØªØ¨Ø±Ø©:** {we.get('num_documents', 0)}\n\n---\n\n"

        # Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø£Ø¯Ø§Ø¡
        md_content += """## 4. Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ø£Ø¯Ø§Ø¡ ÙˆØ§Ù„Ù…ÙˆØ§Ø±Ø¯ (Performance Metrics)

"""

        if self.data['performance_metrics']:
            pm = self.data['performance_metrics']

            if pm.get('document_processing'):
                dp = pm['document_processing']
                md_content += f"""### 4.1 Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª

- **Ù…ØªÙˆØ³Ø· Ø²Ù…Ù† Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©:** {dp.get('avg_time', 0):.2f} Ø«Ø§Ù†ÙŠØ©
- **Ø§Ù„Ø§Ù†Ø­Ø±Ø§Ù Ø§Ù„Ù…Ø¹ÙŠØ§Ø±ÙŠ:** {dp.get('std_time', 0):.2f} Ø«Ø§Ù†ÙŠØ©

"""

            if pm.get('video_processing'):
                vp = pm['video_processing']
                md_content += f"""### 4.2 Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙÙŠØ¯ÙŠÙˆ

- **Ù…ØªÙˆØ³Ø· Ø²Ù…Ù† Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©:** {vp.get('avg_time', 0):.2f} Ø«Ø§Ù†ÙŠØ©
- **Ø§Ù„Ø§Ù†Ø­Ø±Ø§Ù Ø§Ù„Ù…Ø¹ÙŠØ§Ø±ÙŠ:** {vp.get('std_time', 0):.2f} Ø«Ø§Ù†ÙŠØ©

"""

            if pm.get('scalability_test'):
                md_content += """### 4.3 Ù‚Ø§Ø¨Ù„ÙŠØ© Ø§Ù„ØªÙˆØ³Ø¹ (Scalability)

| Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª | Ø²Ù…Ù† Ø§Ù„Ø¨Ø­Ø« (ms) |
|---------------|----------------|
"""
                for test in pm['scalability_test']:
                    md_content += f"| {test['num_documents']} | {test['avg_search_time_ms']:.2f} |\n"

        md_content += """
---

## 5. Ø§Ù„Ø®Ù„Ø§ØµØ© ÙˆØ§Ù„ØªÙˆØµÙŠØ§Øª

### Ø§Ù„Ø¥Ù†Ø¬Ø§Ø²Ø§Øª Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©:

"""

        # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø¥Ù†Ø¬Ø§Ø²Ø§Øª Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        if self.data['search_performance']:
            sp = self.data['search_performance']
            best_f1 = max(sp.get('avg_f1_at_k', {}).values()) if sp.get('avg_f1_at_k') else 0
            md_content += f"âœ… ØªØ­Ù‚ÙŠÙ‚ Ø¯Ù‚Ø© Ø¨Ø­Ø« **{best_f1:.1%}** (F1-Score)\n\n"

        if self.data['video_analysis']:
            va = self.data['video_analysis']
            wer = va.get('avg_wer', 0) * 100
            md_content += f"âœ… Ø¯Ù‚Ø© Ù†Ø³Ø® ØµÙˆØªÙŠ **{100-wer:.1f}%** (WER = {wer:.1f}%)\n\n"

        if self.data['writer_extraction']:
            we = self.data['writer_extraction']
            precision = we.get('avg_precision', 0) * 100
            md_content += f"âœ… Ø¯Ù‚Ø© Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒØªÙ‘Ø§Ø¨ **{precision:.1f}%**\n\n"

        md_content += """
### Ø§Ù„ØªÙˆØµÙŠØ§Øª:

1. **ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù†Ù…Ø§Ø°Ø¬:** Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ù…Ø§Ø°Ø¬ Ø£ÙƒØ¨Ø± ÙˆØ£Ø­Ø¯Ø« Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø¯Ù‚Ø©
2. **Ø§Ù„ØªÙˆØ³Ø¹:** Ø§Ù„Ù†Ø¸Ø§Ù… Ù‚Ø§Ø¨Ù„ Ù„Ù„ØªÙˆØ³Ø¹ Ø­ØªÙ‰ 10,000+ Ù…Ø³ØªÙ†Ø¯
3. **Ø§Ù„ØªØ­Ø³ÙŠÙ†:** ØªÙ‚Ù„ÙŠÙ„ Ø²Ù…Ù† Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¹Ø¨Ø± Ø§Ù„ØªÙˆØ§Ø²ÙŠ ÙˆØ§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª
4. **Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù…Ø³ØªÙ…Ø±:** Ø¥Ø¬Ø±Ø§Ø¡ Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ø¯ÙˆØ±ÙŠØ© Ù„Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ø£Ø¯Ø§Ø¡

---

*ØªÙ… ØªÙˆÙ„ÙŠØ¯ Ù‡Ø°Ø§ Ø§Ù„ØªÙ‚Ø±ÙŠØ± ØªÙ„Ù‚Ø§Ø¦ÙŠÙ‹Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø¨Ø±Ù†Ø§Ù…Ø¬ Ø§Ø®ØªØ¨Ø§Ø± Ù†Ø¸Ø§Ù… RAG API*
"""

        # Ø­ÙØ¸ Ø§Ù„Ù…Ù„Ù
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(md_content)

        logger.info(f"âœ“ ØªÙ… Ø­ÙØ¸ ØªÙ‚Ø±ÙŠØ± Markdown: {output_file}")
        return output_file

    def generate_latex_tables(self) -> Path:
        """ØªÙˆÙ„ÙŠØ¯ Ø¬Ø¯Ø§ÙˆÙ„ LaTeX Ø¬Ø§Ù‡Ø²Ø© Ù„Ù„Ø¨Ø­Ø« Ø§Ù„Ø¹Ù„Ù…ÙŠ"""
        logger.info("ØªÙˆÙ„ÙŠØ¯ Ø¬Ø¯Ø§ÙˆÙ„ LaTeX...")

        output_file = REPORTS_DIR / "latex_tables.tex"

        latex_content = r"""\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{booktabs}
\usepackage{array}

\begin{document}

"""

        # Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ
        if self.data['search_performance']:
            sp = self.data['search_performance']

            latex_content += r"""
\begin{table}[h]
\centering
\caption{Ù…Ù‚Ø§ÙŠÙŠØ³ Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ}
\label{tab:search_performance}
\begin{tabular}{lcccc}
\toprule
\textbf{Ø§Ù„Ù…Ù‚ÙŠØ§Ø³} & \textbf{k=1} & \textbf{k=3} & \textbf{k=5} & \textbf{k=10} \\
\midrule
"""

            # Precision
            latex_content += "Precision & "
            latex_content += " & ".join([
                f"{sp['avg_precision_at_k'].get(str(k), 0):.2f}"
                for k in [1, 3, 5, 10]
            ])
            latex_content += " \\\\\n"

            # Recall
            latex_content += "Recall & "
            latex_content += " & ".join([
                f"{sp['avg_recall_at_k'].get(str(k), 0):.2f}"
                for k in [1, 3, 5, 10]
            ])
            latex_content += " \\\\\n"

            # F1-Score
            latex_content += "F1-Score & "
            latex_content += " & ".join([
                f"{sp['avg_f1_at_k'].get(str(k), 0):.2f}"
                for k in [1, 3, 5, 10]
            ])
            latex_content += " \\\\\n"

            latex_content += r"""\bottomrule
\end{tabular}
\end{table}

"""

        # Ø¬Ø¯ÙˆÙ„ ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ
        if self.data['video_analysis']:
            va = self.data['video_analysis']

            latex_content += r"""
\begin{table}[h]
\centering
\caption{Ø¯Ù‚Ø© ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ}
\label{tab:video_analysis}
\begin{tabular}{lc}
\toprule
\textbf{Ø§Ù„Ù…Ù‚ÙŠØ§Ø³} & \textbf{Ø§Ù„Ù‚ÙŠÙ…Ø©} \\
\midrule
"""

            latex_content += f"WER (\\%) & {va.get('avg_wer', 0)*100:.2f} \\\\\n"
            latex_content += f"CER (\\%) & {va.get('avg_cer', 0)*100:.2f} \\\\\n"
            latex_content += f"ROUGE-1 & {va.get('avg_rouge1', 0):.3f} \\\\\n"
            latex_content += f"ROUGE-2 & {va.get('avg_rouge2', 0):.3f} \\\\\n"
            latex_content += f"ROUGE-L & {va.get('avg_rougeL', 0):.3f} \\\\\n"

            latex_content += r"""\bottomrule
\end{tabular}
\end{table}

"""

        # Ø¬Ø¯ÙˆÙ„ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒØªÙ‘Ø§Ø¨
        if self.data['writer_extraction']:
            we = self.data['writer_extraction']

            latex_content += r"""
\begin{table}[h]
\centering
\caption{Ø£Ø¯Ø§Ø¡ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒØªÙ‘Ø§Ø¨}
\label{tab:writer_extraction}
\begin{tabular}{lc}
\toprule
\textbf{Ø§Ù„Ù…Ù‚ÙŠØ§Ø³} & \textbf{Ø§Ù„Ù†Ø³Ø¨Ø© Ø§Ù„Ù…Ø¦ÙˆÙŠØ©} \\
\midrule
"""

            latex_content += f"Precision & {we.get('avg_precision', 0)*100:.2f}\\% \\\\\n"
            latex_content += f"Recall & {we.get('avg_recall', 0)*100:.2f}\\% \\\\\n"
            latex_content += f"F1-Score & {we.get('avg_f1', 0)*100:.2f}\\% \\\\\n"

            latex_content += r"""\bottomrule
\end{tabular}
\end{table}

"""

        latex_content += r"\end{document}"

        # Ø­ÙØ¸ Ø§Ù„Ù…Ù„Ù
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(latex_content)

        logger.info(f"âœ“ ØªÙ… Ø­ÙØ¸ Ø¬Ø¯Ø§ÙˆÙ„ LaTeX: {output_file}")
        return output_file

    def generate_json_summary(self) -> Path:
        """ØªÙˆÙ„ÙŠØ¯ Ù…Ù„Ø®Øµ JSON Ø´Ø§Ù…Ù„"""
        logger.info("ØªÙˆÙ„ÙŠØ¯ Ù…Ù„Ø®Øµ JSON...")

        output_file = REPORTS_DIR / "summary.json"

        summary = {
            'report_date': self.report_date,
            'search_performance': {
                'precision_at_5': self.data['search_performance'].get('avg_precision_at_k', {}).get('5', 0),
                'recall_at_5': self.data['search_performance'].get('avg_recall_at_k', {}).get('5', 0),
                'f1_at_5': self.data['search_performance'].get('avg_f1_at_k', {}).get('5', 0),
                'mrr': self.data['search_performance'].get('mrr', 0),
                'avg_response_time': self.data['search_performance'].get('avg_response_time', 0)
            },
            'video_analysis': {
                'wer': self.data['video_analysis'].get('avg_wer', 0),
                'cer': self.data['video_analysis'].get('avg_cer', 0),
                'rouge1': self.data['video_analysis'].get('avg_rouge1', 0),
                'rouge2': self.data['video_analysis'].get('avg_rouge2', 0),
                'rougeL': self.data['video_analysis'].get('avg_rougeL', 0)
            },
            'writer_extraction': {
                'precision': self.data['writer_extraction'].get('avg_precision', 0),
                'recall': self.data['writer_extraction'].get('avg_recall', 0),
                'f1': self.data['writer_extraction'].get('avg_f1', 0)
            },
            'performance': {
                'document_processing_time': self.data['performance_metrics'].get('document_processing', {}).get('avg_time', 0),
                'video_processing_time': self.data['performance_metrics'].get('video_processing', {}).get('avg_time', 0)
            }
        }

        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)

        logger.info(f"âœ“ ØªÙ… Ø­ÙØ¸ Ù…Ù„Ø®Øµ JSON: {output_file}")
        return output_file

    def generate_all_reports(self):
        """ØªÙˆÙ„ÙŠØ¯ Ø¬Ù…ÙŠØ¹ Ø§Ù„ØªÙ‚Ø§Ø±ÙŠØ±"""
        logger.info("\n" + "="*80)
        logger.info("Ø¨Ø¯Ø¡ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØªÙ‚Ø§Ø±ÙŠØ± Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ© Ø§Ù„Ø´Ø§Ù…Ù„Ø©")
        logger.info("="*80 + "\n")

        # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        self.load_all_results()

        # ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØªÙ‚Ø§Ø±ÙŠØ±
        md_file = self.generate_markdown_report()
        latex_file = self.generate_latex_tables()
        json_file = self.generate_json_summary()

        logger.info("\n" + "="*80)
        logger.info("âœ… ØªÙ… ØªÙˆÙ„ÙŠØ¯ Ø¬Ù…ÙŠØ¹ Ø§Ù„ØªÙ‚Ø§Ø±ÙŠØ± Ø¨Ù†Ø¬Ø§Ø­!")
        logger.info("="*80)
        logger.info(f"\nØ§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ÙÙ†Ø´Ø£Ø©:")
        logger.info(f"  ğŸ“„ Markdown: {md_file}")
        logger.info(f"  ğŸ“Š LaTeX: {latex_file}")
        logger.info(f"  ğŸ“‹ JSON: {json_file}")
        logger.info(f"\nØ¬Ù…ÙŠØ¹ Ø§Ù„ØªÙ‚Ø§Ø±ÙŠØ± ÙÙŠ: {REPORTS_DIR.absolute()}\n")

        return {
            'markdown': md_file,
            'latex': latex_file,
            'json': json_file
        }


def main():
    """Ø§Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ"""
    print("\n" + "="*80)
    print("ğŸ“Š Ø¨Ø±Ù†Ø§Ù…Ø¬ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØªÙ‚Ø§Ø±ÙŠØ± Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ© Ø§Ù„Ø´Ø§Ù…Ù„Ø©")
    print("="*80 + "\n")

    # Ø¥Ù†Ø´Ø§Ø¡ Ù…ÙˆÙ„Ø¯ Ø§Ù„ØªÙ‚Ø§Ø±ÙŠØ±
    generator = ReportGenerator()

    # ØªÙˆÙ„ÙŠØ¯ Ø¬Ù…ÙŠØ¹ Ø§Ù„ØªÙ‚Ø§Ø±ÙŠØ±
    reports = generator.generate_all_reports()

    print("\nâœ… ØªÙ… Ø§Ù„Ø§Ù†ØªÙ‡Ø§Ø¡ Ø¨Ù†Ø¬Ø§Ø­!")


if __name__ == "__main__":
    main()
