"""
Ø§Ø®ØªØ¨Ø§Ø± Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ø£Ø¯Ø§Ø¡ ÙˆØ§Ù„Ù…ÙˆØ§Ø±Ø¯ - Performance and Resource Metrics Testing
ÙŠÙ‚ÙŠØ³: Ø²Ù…Ù† Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©ØŒ Ø§Ø³ØªÙ‡Ù„Ø§Ùƒ Ø§Ù„Ø°Ø§ÙƒØ±Ø©ØŒ Ø§Ø³ØªØ®Ø¯Ø§Ù… GPUØŒ Ù‚Ø§Ø¨Ù„ÙŠØ© Ø§Ù„ØªÙˆØ³Ø¹
"""

import json
import time
import psutil
import numpy as np
from typing import List, Dict
from pathlib import Path
import logging
import requests
import os

try:
    import GPUtil
    GPU_AVAILABLE = True
except ImportError:
    GPU_AVAILABLE = False
    logging.warning("GPUtil not installed. GPU metrics will not be available.")

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±
API_BASE_URL = "http://localhost:8000"
RESULTS_DIR = Path("test_results")
RESULTS_DIR.mkdir(exist_ok=True)


class PerformanceTester:
    """Ø§Ø®ØªØ¨Ø§Ø± Ø´Ø§Ù…Ù„ Ù„Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ø£Ø¯Ø§Ø¡ ÙˆØ§Ù„Ù…ÙˆØ§Ø±Ø¯"""

    def __init__(self, api_url: str = API_BASE_URL):
        self.api_url = api_url
        self.process = psutil.Process(os.getpid())
        self.results = {
            'document_processing': [],
            'video_processing': [],
            'search_performance': [],
            'scalability_test': [],
            'resource_usage': {
                'cpu': [],
                'memory': [],
                'gpu': []
            }
        }

    def get_resource_usage(self) -> Dict:
        """
        Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…ÙˆØ§Ø±Ø¯ Ø§Ù„Ø­Ø§Ù„ÙŠ
        """
        resources = {
            'cpu_percent': self.process.cpu_percent(interval=0.1),
            'memory_mb': self.process.memory_info().rss / (1024 * 1024),
            'memory_percent': self.process.memory_percent()
        }

        # Ø¥Ø¶Ø§ÙØ© Ù…Ø¹Ù„ÙˆÙ…Ø§Øª GPU Ø¥Ù† ÙˆØ¬Ø¯Øª
        if GPU_AVAILABLE:
            try:
                gpus = GPUtil.getGPUs()
                if gpus:
                    gpu = gpus[0]
                    resources['gpu_memory_used_mb'] = gpu.memoryUsed
                    resources['gpu_memory_total_mb'] = gpu.memoryTotal
                    resources['gpu_utilization_percent'] = gpu.load * 100
            except Exception as e:
                logger.warning(f"Could not get GPU metrics: {e}")

        return resources

    def test_document_processing_time(self, doc_folder: str = "docs"):
        """
        Ø§Ø®ØªØ¨Ø§Ø± Ø²Ù…Ù† Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª
        """
        logger.info("\n" + "="*80)
        logger.info("Ø§Ø®ØªØ¨Ø§Ø± Ø²Ù…Ù† Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª")
        logger.info("="*80)

        if not Path(doc_folder).exists():
            logger.warning(f"Ø§Ù„Ù…Ø¬Ù„Ø¯ {doc_folder} ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯")
            return

        doc_files = list(Path(doc_folder).glob("*.*"))
        if not doc_files:
            logger.warning("Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ù„ÙØ§Øª Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±")
            return

        # Ø§Ø®ØªØ¨Ø§Ø± Ø¹ÙŠÙ†Ø© Ù…Ù† Ø§Ù„Ù…Ù„ÙØ§Øª
        sample_size = min(20, len(doc_files))
        sample_docs = np.random.choice(doc_files, sample_size, replace=False)

        for doc_path in sample_docs:
            doc_name = doc_path.name
            file_size_mb = doc_path.stat().st_size / (1024 * 1024)

            logger.info(f"\nÙ…Ø¹Ø§Ù„Ø¬Ø©: {doc_name} ({file_size_mb:.2f} MB)")

            # Ù‚ÙŠØ§Ø³ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯ Ù‚Ø¨Ù„ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
            resources_before = self.get_resource_usage()

            # Ø¨Ø¯Ø¡ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
            start_time = time.time()

            try:
                # Ù…Ø­Ø§ÙƒØ§Ø© Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø³ØªÙ†Ø¯ (ÙÙŠ Ø­Ø§Ù„Ø© Ø¹Ø¯Ù… ÙˆØ¬ÙˆØ¯ API Ù†Ø´Ø·)
                # ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ¨Ø¯Ø§Ù„ Ù‡Ø°Ø§ Ø¨Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API Ø­Ù‚ÙŠÙ‚ÙŠ
                response = requests.post(
                    f"{self.api_url}/api/refresh",
                    json={"fileName": doc_name, "rebuild_unified": False},
                    timeout=60
                )

                processing_time = time.time() - start_time

                # Ù‚ÙŠØ§Ø³ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯ Ø¨Ø¹Ø¯ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
                resources_after = self.get_resource_usage()

                # Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØºÙŠØ± ÙÙŠ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯
                memory_delta = resources_after['memory_mb'] - resources_before['memory_mb']

                result = {
                    'document': doc_name,
                    'file_size_mb': file_size_mb,
                    'processing_time': processing_time,
                    'memory_delta_mb': memory_delta,
                    'cpu_percent': resources_after['cpu_percent'],
                    'success': response.status_code == 200 if response else False
                }

                self.results['document_processing'].append(result)

                logger.info(f"  Ø²Ù…Ù† Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©: {processing_time:.2f} Ø«Ø§Ù†ÙŠØ©")
                logger.info(f"  Ø§Ø³ØªÙ‡Ù„Ø§Ùƒ Ø§Ù„Ø°Ø§ÙƒØ±Ø©: {memory_delta:.2f} MB")

            except Exception as e:
                logger.error(f"  Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©: {e}")

        self._print_document_processing_summary()

    def test_video_processing_time(self, video_folder: str = "videos"):
        """
        Ø§Ø®ØªØ¨Ø§Ø± Ø²Ù…Ù† Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ù…Ø¹ ØªÙØµÙŠÙ„ Ø§Ù„Ù…Ø±Ø§Ø­Ù„
        """
        logger.info("\n" + "="*80)
        logger.info("Ø§Ø®ØªØ¨Ø§Ø± Ø²Ù…Ù† Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙÙŠØ¯ÙŠÙˆ")
        logger.info("="*80)

        if not Path(video_folder).exists():
            logger.warning(f"Ø§Ù„Ù…Ø¬Ù„Ø¯ {video_folder} ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯")
            return

        video_files = list(Path(video_folder).glob("*.mp4")) + \
                     list(Path(video_folder).glob("*.avi")) + \
                     list(Path(video_folder).glob("*.mov"))

        if not video_files:
            logger.warning("Ù„Ø§ ØªÙˆØ¬Ø¯ ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±")
            return

        # Ø§Ø®ØªØ¨Ø§Ø± Ø¹ÙŠÙ†Ø© Ù…Ù† Ø§Ù„ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª
        sample_size = min(10, len(video_files))
        sample_videos = np.random.choice(video_files, sample_size, replace=False)

        for video_path in sample_videos:
            video_name = video_path.name
            file_size_mb = video_path.stat().st_size / (1024 * 1024)

            logger.info(f"\nØªØ­Ù„ÙŠÙ„ ÙÙŠØ¯ÙŠÙˆ: {video_name} ({file_size_mb:.2f} MB)")

            # Ù‚ÙŠØ§Ø³ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯ Ù‚Ø¨Ù„ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
            resources_before = self.get_resource_usage()

            # Ø¨Ø¯Ø¡ Ø§Ù„ØªØ­Ù„ÙŠÙ„
            start_time = time.time()

            try:
                response = requests.post(
                    f"{self.api_url}/api/video/analyze_existing",
                    json={
                        "video_filename": video_name,
                        "num_frames": 10,
                        "output_language": "arabic"
                    },
                    timeout=300
                )

                processing_time = time.time() - start_time

                # Ù‚ÙŠØ§Ø³ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯ Ø¨Ø¹Ø¯ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
                resources_after = self.get_resource_usage()

                result = {
                    'video': video_name,
                    'file_size_mb': file_size_mb,
                    'total_processing_time': processing_time,
                    'memory_delta_mb': resources_after['memory_mb'] - resources_before['memory_mb'],
                    'cpu_percent': resources_after['cpu_percent'],
                    'success': response.status_code == 200 if response else False
                }

                if response and response.status_code == 200:
                    data = response.json()
                    result['num_frames_analyzed'] = data.get('num_frames_analyzed', 0)
                    result['detected_language'] = data.get('detected_language', 'unknown')

                self.results['video_processing'].append(result)

                logger.info(f"  Ø²Ù…Ù† Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙƒÙ„ÙŠ: {processing_time:.2f} Ø«Ø§Ù†ÙŠØ©")
                logger.info(f"  Ø§Ø³ØªÙ‡Ù„Ø§Ùƒ Ø§Ù„Ø°Ø§ÙƒØ±Ø©: {result['memory_delta_mb']:.2f} MB")

            except Exception as e:
                logger.error(f"  Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ­Ù„ÙŠÙ„: {e}")

        self._print_video_processing_summary()

    def test_search_scalability(self, num_documents_list: List[int] = None):
        """
        Ø§Ø®ØªØ¨Ø§Ø± Ù‚Ø§Ø¨Ù„ÙŠØ© Ø§Ù„ØªÙˆØ³Ø¹ - Ø²Ù…Ù† Ø§Ù„Ø¨Ø­Ø« Ù…Ø¹ Ø£Ø­Ø¬Ø§Ù… Ù…Ø®ØªÙ„ÙØ©
        """
        logger.info("\n" + "="*80)
        logger.info("Ø§Ø®ØªØ¨Ø§Ø± Ù‚Ø§Ø¨Ù„ÙŠØ© Ø§Ù„ØªÙˆØ³Ø¹ (Scalability)")
        logger.info("="*80)

        if num_documents_list is None:
            num_documents_list = [100, 500, 1000, 5000, 10000]

        test_queries = [
            "renewable energy",
            "Ø§Ù„Ø·Ø§Ù‚Ø© Ø§Ù„Ù…ØªØ¬Ø¯Ø¯Ø©",
            "economic development",
            "Ø§Ù„ØªØ·ÙˆØ±Ø§Øª Ø§Ù„Ø§Ù‚ØªØµØ§Ø¯ÙŠØ©"
        ]

        for num_docs in num_documents_list:
            logger.info(f"\nØ§Ø®ØªØ¨Ø§Ø± Ù…Ø¹ {num_docs} Ù…Ø³ØªÙ†Ø¯...")

            search_times = []

            for query in test_queries:
                start_time = time.time()

                try:
                    response = requests.post(
                        f"{self.api_url}/api/search",
                        json={
                            "query": query,
                            "search_mode": "unified",
                            "top_k": 5
                        },
                        timeout=30
                    )

                    search_time = time.time() - start_time

                    if response.status_code == 200:
                        search_times.append(search_time * 1000)  # ØªØ­ÙˆÙŠÙ„ Ù„Ù…ÙŠÙ„ÙŠ Ø«Ø§Ù†ÙŠØ©

                except Exception as e:
                    logger.error(f"  Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø¨Ø­Ø«: {e}")

            if search_times:
                avg_search_time = np.mean(search_times)
                std_search_time = np.std(search_times)

                result = {
                    'num_documents': num_docs,
                    'avg_search_time_ms': avg_search_time,
                    'std_search_time_ms': std_search_time,
                    'num_queries': len(search_times)
                }

                self.results['scalability_test'].append(result)

                logger.info(f"  Ù…ØªÙˆØ³Ø· Ø²Ù…Ù† Ø§Ù„Ø¨Ø­Ø«: {avg_search_time:.2f} ms (Â±{std_search_time:.2f})")

        self._print_scalability_summary()

    def test_concurrent_requests(self, num_requests: int = 10):
        """
        Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø·Ù„Ø¨Ø§Øª Ø§Ù„Ù…ØªØ²Ø§Ù…Ù†Ø©
        """
        logger.info("\n" + "="*80)
        logger.info(f"Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø·Ù„Ø¨Ø§Øª Ø§Ù„Ù…ØªØ²Ø§Ù…Ù†Ø© ({num_requests} Ø·Ù„Ø¨)")
        logger.info("="*80)

        import concurrent.futures

        def make_search_request():
            start = time.time()
            try:
                response = requests.post(
                    f"{self.api_url}/api/search",
                    json={"query": "test query", "search_mode": "unified", "top_k": 5},
                    timeout=30
                )
                elapsed = time.time() - start
                return {'success': response.status_code == 200, 'time': elapsed}
            except Exception as e:
                return {'success': False, 'time': 0, 'error': str(e)}

        # ØªÙ†ÙÙŠØ° Ø§Ù„Ø·Ù„Ø¨Ø§Øª Ø§Ù„Ù…ØªØ²Ø§Ù…Ù†Ø©
        start_time = time.time()

        with concurrent.futures.ThreadPoolExecutor(max_workers=num_requests) as executor:
            futures = [executor.submit(make_search_request) for _ in range(num_requests)]
            results = [f.result() for f in concurrent.futures.as_completed(futures)]

        total_time = time.time() - start_time

        successful = sum(1 for r in results if r['success'])
        avg_response_time = np.mean([r['time'] for r in results if r['success']])

        logger.info(f"  Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„ÙˆÙ‚Øª: {total_time:.2f} Ø«Ø§Ù†ÙŠØ©")
        logger.info(f"  Ø·Ù„Ø¨Ø§Øª Ù†Ø§Ø¬Ø­Ø©: {successful}/{num_requests}")
        logger.info(f"  Ù…ØªÙˆØ³Ø· Ø²Ù…Ù† Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø©: {avg_response_time:.2f} Ø«Ø§Ù†ÙŠØ©")
        logger.info(f"  Ù…Ø¹Ø¯Ù„ Ø§Ù„Ø¥Ù†ØªØ§Ø¬ÙŠØ©: {num_requests/total_time:.2f} Ø·Ù„Ø¨/Ø«Ø§Ù†ÙŠØ©")

        return {
            'num_requests': num_requests,
            'total_time': total_time,
            'successful_requests': successful,
            'avg_response_time': avg_response_time,
            'throughput': num_requests / total_time
        }

    def _print_document_processing_summary(self):
        """Ø·Ø¨Ø§Ø¹Ø© Ù…Ù„Ø®Øµ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª"""
        if not self.results['document_processing']:
            return

        times = [r['processing_time'] for r in self.results['document_processing']]
        sizes = [r['file_size_mb'] for r in self.results['document_processing']]

        logger.info("\nğŸ“Š Ù…Ù„Ø®Øµ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª:")
        logger.info(f"  Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª: {len(times)}")
        logger.info(f"  Ù…ØªÙˆØ³Ø· Ø§Ù„Ø²Ù…Ù†: {np.mean(times):.2f} Ø«Ø§Ù†ÙŠØ©")
        logger.info(f"  Ø§Ù„Ø§Ù†Ø­Ø±Ø§Ù Ø§Ù„Ù…Ø¹ÙŠØ§Ø±ÙŠ: {np.std(times):.2f} Ø«Ø§Ù†ÙŠØ©")
        logger.info(f"  Ù…ØªÙˆØ³Ø· Ø­Ø¬Ù… Ø§Ù„Ù…Ù„Ù: {np.mean(sizes):.2f} MB")

    def _print_video_processing_summary(self):
        """Ø·Ø¨Ø§Ø¹Ø© Ù…Ù„Ø®Øµ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙÙŠØ¯ÙŠÙˆ"""
        if not self.results['video_processing']:
            return

        times = [r['total_processing_time'] for r in self.results['video_processing']]
        sizes = [r['file_size_mb'] for r in self.results['video_processing']]

        logger.info("\nğŸ“Š Ù…Ù„Ø®Øµ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙÙŠØ¯ÙŠÙˆ:")
        logger.info(f"  Ø¹Ø¯Ø¯ Ø§Ù„ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª: {len(times)}")
        logger.info(f"  Ù…ØªÙˆØ³Ø· Ø§Ù„Ø²Ù…Ù†: {np.mean(times):.2f} Ø«Ø§Ù†ÙŠØ©")
        logger.info(f"  Ø§Ù„Ø§Ù†Ø­Ø±Ø§Ù Ø§Ù„Ù…Ø¹ÙŠØ§Ø±ÙŠ: {np.std(times):.2f} Ø«Ø§Ù†ÙŠØ©")
        logger.info(f"  Ù…ØªÙˆØ³Ø· Ø­Ø¬Ù… Ø§Ù„Ù…Ù„Ù: {np.mean(sizes):.2f} MB")

    def _print_scalability_summary(self):
        """Ø·Ø¨Ø§Ø¹Ø© Ù…Ù„Ø®Øµ Ù‚Ø§Ø¨Ù„ÙŠØ© Ø§Ù„ØªÙˆØ³Ø¹"""
        if not self.results['scalability_test']:
            return

        logger.info("\nğŸ“Š Ù…Ù„Ø®Øµ Ù‚Ø§Ø¨Ù„ÙŠØ© Ø§Ù„ØªÙˆØ³Ø¹:")
        logger.info("-" * 60)
        logger.info(f"{'Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª':<20} {'Ø²Ù…Ù† Ø§Ù„Ø¨Ø­Ø« (ms)':<20}")
        logger.info("-" * 60)

        for result in self.results['scalability_test']:
            logger.info(f"{result['num_documents']:<20} {result['avg_search_time_ms']:>15.2f}")

        logger.info("-" * 60)

    def save_results(self, filename: str = "performance_metrics.json"):
        """Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø¥Ù„Ù‰ Ù…Ù„Ù JSON"""
        output_path = RESULTS_DIR / filename

        results_to_save = {
            'document_processing': {
                'avg_time': float(np.mean([r['processing_time'] for r in self.results['document_processing']])) if self.results['document_processing'] else None,
                'std_time': float(np.std([r['processing_time'] for r in self.results['document_processing']])) if self.results['document_processing'] else None,
                'details': self.results['document_processing']
            },
            'video_processing': {
                'avg_time': float(np.mean([r['total_processing_time'] for r in self.results['video_processing']])) if self.results['video_processing'] else None,
                'std_time': float(np.std([r['total_processing_time'] for r in self.results['video_processing']])) if self.results['video_processing'] else None,
                'details': self.results['video_processing']
            },
            'scalability_test': self.results['scalability_test'],
            'resource_usage': self.results['resource_usage']
        }

        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(results_to_save, f, ensure_ascii=False, indent=2)

        logger.info(f"\nâœ… ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙÙŠ: {output_path}")
        return output_path


def main():
    """Ø§Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ"""
    print("\n" + "="*80)
    print("âš¡ Ø¨Ø±Ù†Ø§Ù…Ø¬ Ø§Ø®ØªØ¨Ø§Ø± Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ø£Ø¯Ø§Ø¡ ÙˆØ§Ù„Ù…ÙˆØ§Ø±Ø¯")
    print("="*80 + "\n")

    # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø«ÙŠÙ„ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±
    tester = PerformanceTester()

    # 1. Ø§Ø®ØªØ¨Ø§Ø± Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª
    tester.test_document_processing_time()

    # 2. Ø§Ø®ØªØ¨Ø§Ø± Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙÙŠØ¯ÙŠÙˆ
    tester.test_video_processing_time()

    # 3. Ø§Ø®ØªØ¨Ø§Ø± Ù‚Ø§Ø¨Ù„ÙŠØ© Ø§Ù„ØªÙˆØ³Ø¹
    tester.test_search_scalability([100, 500, 1000, 5000])

    # 4. Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø·Ù„Ø¨Ø§Øª Ø§Ù„Ù…ØªØ²Ø§Ù…Ù†Ø©
    tester.test_concurrent_requests(num_requests=20)

    # Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
    tester.save_results()

    print("\nâœ… Ø§ÙƒØªÙ…Ù„Øª Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ø¨Ù†Ø¬Ø§Ø­!")


if __name__ == "__main__":
    main()
